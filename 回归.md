## 线性回归

### 回归模型

假定回归模型
$$
Y=b_{0}+b_{1} X+e
$$
$e$ 为随机误差
$$
E(e)=0\\
Var(e)=\sigma^2
$$

### 中心化

$$
Y_{i}=\beta_{0}+\beta_{1}\left(X_{i}-\bar{X}\right)+e_{i}, i=1, \cdots, n \tag{1.1}
$$

其中
$$
\beta_{1}=b_{1}, \beta_{0}=b_{0}+b_{1} \bar{X}
$$

### 参数估计

用 $\alpha_0$ 和 $\alpha_1$ 估计 $\beta$  
$$
\hat{Y}_{i}=\alpha_{0}+\alpha_{1}\left(X_{i}-\bar{X}\right), i=1, \cdots, n \tag{1.2}
$$
损失函数
$$
Q\left(\alpha_{0}, \alpha_{1}\right)=\sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}=\sum_{i=1}^{n}\left[Y_{i}-\alpha_{0}-\alpha_{1}\left(X_{i}-\bar{X}\right)\right]^{2}
$$
求
$$
\begin{aligned}
&\frac{\partial Q}{\partial \alpha_{0}}=-2 \sum_{i=1}^{n}\left(Y_{i}-\alpha_{0}-\alpha_{1}\left(X_{i}-\bar{X}\right)\right)=0\\
&\frac{\partial Q}{\partial \alpha_{1}}=-2 \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left[Y_{i}-\alpha_{0}-\alpha_{1}\left(X_{i}-\bar{X}\right)\right]=0
\end{aligned}
$$
注意到
$$
\sum_{i=1}^{n}\left(Y_{i}-\alpha_{0}-\alpha_{1}\left(X_{i}-\bar{X}\right)\right)=\sum_{i=1}^{n}Y_{i}-\alpha_{0}-\sum_{i=1}^{n}\alpha_{1}\left(X_{i}-\bar{X}\right)=\sum_{i=1}^{n}Y_{i}-\alpha_{0}
$$
所以
$$
a_0=\hat{\beta_0}=\bar{Y}=\frac{1}{n} \sum_{i=1}^{n}\left(\beta_{0}+\beta_{1}\left(X_{i}-\bar{X}\right)+e_{i}\right)=\beta_0+\bar{e} \tag{1.3}
$$
代入下方
$$
\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left[Y_{i}-\bar{Y}\right]=\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\alpha_{1}\left(X_{i}-\bar{X}\right)
$$
所以
$$
a_1=\hat{\beta_1}=\frac{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) Y _ { i } }{ \sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) ^ { 2 }}=\frac{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) e _ { i } }{ \sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) ^ { 2 }}+\beta_1 \tag{1.4}
$$

#### 估计的性质

这个估计是无偏的
$$
E(\hat{\beta_0})=\beta_0+E(\bar{e})= \beta _ { 0 } 
$$

$$
E(\hat{\beta_1})=\frac{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right)E( Y _ { i } )}{ \sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) ^ { 2 }}=\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left[\beta_{0}+\beta_{1}\left(X_{i}-\bar{X}\right)\right] / \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=\beta_1
$$

方差为
$$
Var(\hat{\beta_1})=\frac{\sigma^2}{n}
$$

$$
Var({\beta_1})=\frac{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right)^2Var( Y _ { i }) }{\left[ \sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) ^ { 2 }\right]^2}=\frac{\sigma^2}{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right)^2}
$$

且两者不相关:
$$
E(\hat{\beta_0}\hat{\beta_1})=E(\frac{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) Y _ { i }\bar{Y} }{ \sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) ^ { 2 }})
$$

$$
E(\sum _ { i = 1 } ^ { n }   Y _ { i }\bar{Y} )=E[(\beta_0+\beta_1(X_i-\bar{X})+e)\bar{Y}]\\=\beta_0E(\hat{Y})+\beta_1(X_i-\bar{X})E(\hat{Y})+E(e\hat{Y})=\beta_0E(\hat{Y})+\beta_1(X_i-\bar{X})E(\hat{Y})
$$

所以
$$
E(\hat{\beta_0}\hat{\beta_1})=\frac{\sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) [\beta_0E(\hat{Y})+\beta_1(X_i-\bar{X})E(\hat{Y})] }{ \sum _ { i = 1 } ^ { n } \left( X _ { i } - \bar { X } \right) ^ { 2 }}=\beta_1E(\hat{Y})=\beta_0\beta_1
$$

### 残差

预测与观测之差为残差:
$$
\delta_{i}=Y_{i}-\hat{Y}_{i}, i=1, \cdots, n
$$
残差可以用于估计 $\sigma^2$:
$$
\delta_i=Y_{i}-\hat{Y}_{i}=\beta_{0}-\hat{\beta}_{0}+(\beta_{1}-\hat{\beta_{1}})\left(X_{i}-\bar{X}\right)+e_{i}
$$
其中
$$
\beta_{0}-\hat{\beta}_{0}=-\bar{e}
$$

$$
\beta_{1}-\hat{\beta_{1}}=-\frac{\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) e _ { j } }{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}
$$

所以
$$
\delta_i=e_i-\bar{e}-(X_i-\bar{X})
\frac{\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) e _ { j } }{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}
$$
注意到
$$
\sum_{i=1}^{n}\left[(X_i-\bar{X})\frac{\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) e _ { j } }{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}\right]^2
=\frac{\left[\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right)e_j \right]^2\sum_{i=1}^{n}(X_i-\bar{X})^2}{\left[ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }\right]^2}
=\frac{\left[\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right)e_j \right]^2}{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}
$$
且
$$
\sum_{i=1}^{n}\left(e_{i}-\bar{e}\right)\left(X_{i}-\bar{X}\right)=\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right) e_{i}
$$
所以
$$
\sum_{i=1}^{n}\left(e_{i}-\bar{e}\right)\left(X_{i}-\bar{X}\right)\frac{\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) e _ { j } }{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}
=\frac{\left[\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right)e_j \right]^2}{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}
$$
所以
$$
\sum_{i=1}^{n} \delta_{i}^{2}=

\sum_{i=1}^{n}\left(e_{i}-\bar{e}\right)^{2}

-\frac{\left[\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right)e_j \right]^2}{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }} \tag{2.1}
$$
由中央极限定理 
$$
\frac{\left[\sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right)e_j \right]^2}{ \sum _ { j = 1 } ^ { n } \left( X _ { j } - \bar { X } \right) ^ { 2 }}/\sigma^2\sim \chi^{2}_{}\\
\sum_{i=1}^{n}\left(e_{i}-\bar{e}\right)^{2}/\sigma^2\sim\chi^{2}_{n-1}
$$
所以
$$
\sum_{i=1}^{n} \delta_{i}^{2} / \sigma^{2} \sim \chi_{n-2}^{2} \tag{2.2}
$$
从而
$$
E\left(\sum_{i=1}^{n} \delta_{i}^{2}\right)=(n-2) \sigma^{2}
$$

### 区间估计和假设检验

假设 $e$ 是 i.i.d 的
$$
e_i \sim N(0,\sigma^2)
$$
则
$$
\frac{\hat{\beta}_{1}-\beta_{1}}{\hat{\sigma} S_{x}^{-1}}\sim t_{n-2}
$$

## 多元线性回归

考虑中心化后的关系式
$$
Y_{i}=\beta_{0}+\beta_{1} X_{1 i}+\cdots+\beta_{p} X_{p i}+e_{i}, i=1, \cdots, n 
$$
由于中心化, 对于任意的 $p$
$$
\sum_{i=1}^{n}X_{pi}=0
$$
所以
$$
\left\{\begin{array}{l}
{l_{11} \alpha_{1}+l_{12} \alpha_{2}+\cdots+l_{1 p} \alpha_{p}=\sum_{i=1}^{n} X_{1 i} Y_{i}} \\
{\cdots \cdots \cdots \cdots} \\
{l_{p 1} \alpha_{1}+l_{p 2} \alpha_{2}+\cdots+l_{p p} \alpha_{p}=\sum_{i=1}^{n} X_{p i} Y_{i}}
\end{array}\right.
$$
其中, $l_{uv}=\sum_{i=1}^{n} X_{u i} X_{v i}$ 且有
$$
\alpha_0=\hat{\beta_0}=\bar{Y}=\beta_0+\bar{e}
$$
则方程组可以改写为
$$
L=XX^T\\
L\alpha=XY
$$
其中
$$
Y=\left(\begin{array}{l}
{Y_{1}} \\
{\vdots} \\
{Y_{n}}
\end{array}\right),
\beta=\left(\begin{array}{l}
{\beta_{1}} \\
{\vdots} \\
{\beta_{p}}
\end{array}\right)
,
X=\left(\begin{array}{cccc}
{X_{11}} & {X_{12}} & {\cdots} & {X_{1 n}} \\
{X_{21}} & {X_{22}} & {\cdots} & {X_{2 n}} \\
{\cdots} & {\cdots} & {\cdots} & {\cdots} \\
{X_{p 1}} & {X_{p 2}} & {\cdots} & {X_{\beta n}}
\end{array}\right), L=\left(\begin{array}{cccc}
{l_{11}} & {l_{12}} & {\cdots} & {l_{1 p}} \\
{l_{21}} & {l_{22}} & {\cdots} & {l_{2 p}} \\
{\cdots} & {\cdots} & {\cdots} & {\cdots} \\
{l_{p 1}} & {l_{p 2}} & {\cdots} & {l_{p p}}
\end{array}\right)
$$

$$
\hat{\beta}=L^{-1}XY
$$

注意到
$$
E(\hat{\beta_0}\hat{\beta_i})=E((\beta_0+\bar{e})\hat{\beta_i})=\beta_0\beta_i
$$
所以
$$
Cov(\hat{\beta_0},\hat{\beta_i})=0
$$
且有
$$
Var(\hat{\beta_j})=L^{-1}_{jj}\sigma^2\\
Cov(\hat{\beta_j},\hat{\beta_k})=L^{-1}_{jk}\sigma^2
$$

$$
\sum_{i=1}^{n} \delta_{i}^{2} / \sigma^{2} \sim \chi_{n-p-1}^{2}
$$

