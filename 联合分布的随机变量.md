### 联合分布的随机变量

对于任两个随机变量 $X$ 和 $Y$, 我们定义它们的**联合累计概率分布函数**为:
$$
F(a, b)=P\{X \leqslant a, Y \leqslant b\}, \quad-\infty<a, b<\infty
$$
$X$ 和 $Y$ 的累计分布可以如下得到:
$$
\begin{aligned} F_{X}(a) &=P\{X \leqslant a\} \\ &=P\{X \leqslant a, Y<\infty\} \\ &=F(a, \infty) \end{aligned}
$$

$$
F_{Y}(b)=P\{Y \leqslant b\}=F(\infty, b)
$$

In the case where $X$ and $Y$ are both discrete random variables, define the *the joint probability mass function* of $X$ and $Y$ by:
$$
p(x, y)=P\{X=x, Y=y\}
$$
The probability mass function of $X$ and $Y$ may be obtained from $p(x, y)$ by:
$$
p_{X}(x)=\sum_{y : p(x, y)>0} p(x, y)\\
p_{Y}(y)=\sum_{x : p(x, y)>0} p(x, y)
$$
We say that $X$ and $Y$ are **jointly continuous** if there exists a function $f(x, y)$, defined for all real $x$ and $y$, having the property that for all sets $A$ and $B$ of real numbers
$$
P\{X \in A, Y \in B\}=\int_{B} \int_{A} f(x, y) d x d y
$$
The function $f(x, y)$ is called the joint probability density function of $X$ and $Y$. The probability density of X can be obtained as the following:
$$
\begin{aligned} P\{X \in A\} &=P\{X \in A, Y \in(-\infty, \infty)\} \\ &=\int_{-\infty}^{\infty} \int_{A} f(x, y) d x d y \\ &=\int_{A} f_{X}(x) d x \end{aligned}
$$
其中交换了积分:
$$
f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) d y
$$
类似地
$$
f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) d x
$$
从定义得:
$$
\frac{\mathrm{d}^{2}}{\mathrm{d} a \mathrm{d} b} F(a, b)=f(a, b)
$$
Then
$$
\begin{aligned} E[g(X, Y)] &=\sum_{y} \sum_{x} g(x, y) p(x, y)\quad \quad \quad \quad\text { in the discrete case } \\ &=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f(x, y) d x d y \quad \text { in the continuous case } \end{aligned}
$$


如果
$$
\mathrm{P}\{X \leqslant a, Y \leqslant b\}=\mathrm{P}\{X \leqslant a\} \mathrm{P}\{Y \leqslant b\}
$$
那么称 $X$ 和 $Y$ 是独立的.

在连续情况下,
$$
\begin{aligned} \mathrm{P}\{X \leqslant a, Y \leqslant b\} &=\int_{-\infty}^{a} \int_{-\infty}^{b} f(x, y) \mathrm{d} y \mathrm{d} x \end{aligned}
$$
另一方面:
$$
P\{Y \leqslant b\} P\{X \leqslant a\}=\int_{-\infty}^{b}f_Y(y)\mathrm{d} y\int_{-\infty}^{a}f_X(x)\mathrm{d} x
$$
这意味着
$$
f(x, y)=f_{X}(x) f_{Y}(y)
$$
类似地, 在离散情况下
$$
p(x, y)=p_{X}(x) p_{Y}(y)
$$


如果 $X$ 和 $Y$ 独立, 那么对于任意函数 $g$ 和 $h$, 容易证明  
$$
\mathrm{E}[g(X) h(Y)]=\mathrm{E}[g(X)] \mathrm{E}[h(Y)]
$$


为了描述两个随机变量地相关程度, 我们定义随机变量 $X$ 和 $Y​$ 的协方差为
$$
\begin{aligned} \operatorname{Cov}(X, Y) &=\mathrm{E}[(X-\mathrm{E}[X])(Y-\mathrm{E}[Y])] \\ &=\mathrm{E}[X Y-Y \mathrm{E}[X]-X \mathrm{E}[Y]+\mathrm{E}[X] \mathrm{E}[Y]] \\ &=\mathrm{E}[X Y]-\mathrm{E}[Y] \mathrm{E}[X]-\mathrm{E}[X] \mathrm{E}[Y]+\mathrm{E}[X] \mathrm{E}[Y] \\ &=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y] \end{aligned}
$$
为了直观体现协方差的性质, 取 $X$ 和 $Y$ 为事件 $A$ 和 $B$   是否发生的示性函数.
$$
\operatorname{Cov}(X, Y)=\mathrm{P}\{X=1, Y=1\}-\mathrm{P}\{X=1\} \mathrm{P}\{Y=1\}
$$
所以
$$
\begin{aligned} \operatorname{Cov}(X, Y)>0 & \Leftrightarrow \mathrm{P}\{X=1, Y=1\}>\{X=1\} \mathrm{P}\{Y=1\} \\ & \Leftrightarrow \frac{\mathrm{P}\{X=1, Y=1\}}{P\{X=1\}}>\mathrm{P}\{Y=1\} \\ & \Leftrightarrow \mathrm{P}\{Y=1 | X=1\}>\mathrm{P}\{Y=1\} \end{aligned}
$$
就是说 $\operatorname{Cov}(X, Y)>0$ 意味着当 $X$ 增加, $Y$ 倾向于增加.

对于协方差, 可以证明一些性质:
$$
\operatorname{Cov}(X, X)=E[X^2]-E[X]^2=\operatorname{Var}(X)
$$

$$
\operatorname{Cov}(X, Y)=\operatorname{Cov}(Y, X)
$$

$$
\operatorname{Cov}(c X, Y)=\operatorname{cCov}(X, Y)
$$

$$
\begin{aligned} \operatorname{Cov}(X, Y+Z) &=\mathrm{E}[X(Y+Z)]-\mathrm{E}[X] \mathrm{E}[Y+Z] \\ &=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y]+\mathrm{E}[X Z]-\mathrm{E}[X] \mathrm{E}[Z] \\ &=\operatorname{Cov}(X, Y)+\operatorname{Cov}(X, Z) \end{aligned}
$$

所以可以证明
$$
\operatorname{Cov}\left(X, \sum_{j=1}^{m} Y_{j}\right)=\sum_{j=1}^{m} \operatorname{Cov}\left(X_{}, Y_{j}\right)
$$
更进一步
$$
\operatorname{Cov}\left(\sum_{i=1}^{n} X_{i}, \sum_{j=1}^{m} Y_{j}\right)=\sum_{i=1}^{n} \sum_{j=1}^{m} \operatorname{Cov}\left(X_{i}, Y_{j}\right)
$$
利用该性质:
$$
\begin{aligned} \operatorname{Var}\left(\sum_{i=1}^{n} X_{i}\right) &=\operatorname{Cov}\left(\sum_{i=1}^{n} X_{i}, \sum_{j=1}^{n} X_{j}\right)=\sum_{i=1}^{n} \sum_{j=1}^{n} \operatorname{Cov}\left(X_{i}, X_{j}\right) \\ &=\sum_{i=1}^{n} \operatorname{Cov}\left(X_{i}, X_{i}\right)+\sum_{i=1}^{n} \sum_{j \neq i} \operatorname{Cov}\left(X_{i}, X_{j}\right) \\ &=\sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)+2 \sum_{i=1}^{n} \sum_{j<i} \operatorname{Cov}\left(X_{i}, X_{j}\right) \end{aligned}
$$
如果 $X_i​$ 均是独立随机变量, 那么
$$
\operatorname{Var}\left(\sum_{i=1}^{n} X_{i}\right)=\sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)
$$
若 $X_1,\ldots,X_n$ i.i.d, 则随机变量 $\overline{X}=\sum_{i=1}^{n} X_{i} / n$ 被称为**样本均值**.
$$
\mathrm{E}[\overline{X}]=\mathrm{E}[\sum_{i=1}^{n} X_{i} / n]=\sum_{i=1}^{n}E[X_i]/n=\sum_{i=1}^{n}\mu/n=\mu
$$

$$
\operatorname{Var}(\overline{X})=\left(\frac{1}{n}\right)^{2} \operatorname{Var}\left(\sum_{i=1}^{n} X_{i}\right)=\left(\frac{1}{n}\right)^{2} \sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)=\frac{\sigma^{2}}{n}
$$

显然, 样本均值的具体大小和样本与均值间的偏差没有关系, 这可以如下证明
$$
\begin{aligned} \operatorname{Cov}\left(\overline{X}, X_{i}-\overline{X}\right) &=\operatorname{Cov}\left(\overline{X}, X_{i}\right)-\operatorname{Cov}(\overline{X}, \overline{X}) \\ &=\frac{1}{n} \operatorname{Cov}\left(X_{i}+\sum_{j \neq i} X_{j}, X_{i}\right)-\operatorname{Var}(\overline{X}) \\ &=\frac{1}{n} \operatorname{Cov}\left(X_{i}, X_{i}\right)+\frac{1}{n} \operatorname{Cov}\left(\sum_{j \neq i} X_{j}, X_{i}\right)-\frac{\sigma^{2}}{n} \\ &=\frac{\sigma^{2}}{n}-\frac{\sigma^{2}}{n}=0 \end{aligned}
$$
