### 联合分布的随机变量

对于任两个随机变量 $X$ 和 $Y$, 我们定义它们的**联合累计概率分布函数**为:
$$
F(a, b)=P\{X \leqslant a, Y \leqslant b\}, \quad-\infty<a, b<\infty
$$
$X$ 和 $Y$ 的累计分布可以如下得到:
$$
\begin{aligned} F_{X}(a) &=P\{X \leqslant a\} \\ &=P\{X \leqslant a, Y<\infty\} \\ &=F(a, \infty) \end{aligned}
$$

$$
F_{Y}(b)=P\{Y \leqslant b\}=F(\infty, b)
$$

In the case where $X$ and $Y$ are both discrete random variables, define the *the joint probability mass function* of $X$ and $Y$ by:
$$
p(x, y)=P\{X=x, Y=y\}
$$
The probability mass function of $X$ and $Y$ may be obtained from $p(x, y)$ by:
$$
p_{X}(x)=\sum_{y : p(x, y)>0} p(x, y)\\
p_{Y}(y)=\sum_{x : p(x, y)>0} p(x, y)
$$
We say that $X$ and $Y$ are **jointly continuous** if there exists a function $f(x, y)$, defined for all real $x$ and $y$, having the property that for all sets $A$ and $B$ of real numbers
$$
P\{X \in A, Y \in B\}=\int_{B} \int_{A} f(x, y) d x d y
$$
The function $f(x, y)$ is called the joint probability density function of $X$ and $Y$. The probability density of X can be obtained as the following:
$$
\begin{aligned} P\{X \in A\} &=P\{X \in A, Y \in(-\infty, \infty)\} \\ &=\int_{-\infty}^{\infty} \int_{A} f(x, y) d x d y \\ &=\int_{A} f_{X}(x) d x \end{aligned}
$$
其中交换了积分:
$$
f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) d y
$$
类似地
$$
f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) d x
$$
从定义得:
$$
\frac{\mathrm{d}^{2}}{\mathrm{d} a \mathrm{d} b} F(a, b)=f(a, b)
$$
Then
$$
\begin{aligned} E[g(X, Y)] &=\sum_{y} \sum_{x} g(x, y) p(x, y)\quad \quad \quad \quad\text { in the discrete case } \\ &=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f(x, y) d x d y \quad \text { in the continuous case } \end{aligned}
$$


如果
$$
\mathrm{P}\{X \leqslant a, Y \leqslant b\}=\mathrm{P}\{X \leqslant a\} \mathrm{P}\{Y \leqslant b\}
$$
那么称 $X$ 和 $Y$ 是独立的.

在连续情况下,
$$
\begin{aligned} \mathrm{P}\{X \leqslant a, Y \leqslant b\} &=\int_{-\infty}^{a} \int_{-\infty}^{b} f(x, y) \mathrm{d} y \mathrm{d} x \end{aligned}
$$
另一方面:
$$
P\{Y \leqslant b\} P\{X \leqslant a\}=\int_{-\infty}^{b}f_Y(y)\mathrm{d} y\int_{-\infty}^{a}f_X(x)\mathrm{d} x
$$
这意味着
$$
f(x, y)=f_{X}(x) f_{Y}(y)
$$
类似地, 在离散情况下
$$
p(x, y)=p_{X}(x) p_{Y}(y)
$$


如果 $X$ 和 $Y$ 独立, 那么对于任意函数 $g$ 和 $h$, 容易证明  
$$
\mathrm{E}[g(X) h(Y)]=\mathrm{E}[g(X)] \mathrm{E}[h(Y)]
$$


为了描述两个随机变量地相关程度, 我们定义随机变量 $X$ 和 $Y​$ 的协方差为
$$
\begin{aligned} \operatorname{Cov}(X, Y) &=\mathrm{E}[(X-\mathrm{E}[X])(Y-\mathrm{E}[Y])] \\ &=\mathrm{E}[X Y-Y \mathrm{E}[X]-X \mathrm{E}[Y]+\mathrm{E}[X] \mathrm{E}[Y]] \\ &=\mathrm{E}[X Y]-\mathrm{E}[Y] \mathrm{E}[X]-\mathrm{E}[X] \mathrm{E}[Y]+\mathrm{E}[X] \mathrm{E}[Y] \\ &=\mathrm{E}[X Y]-\mathrm{E}[X] \mathrm{E}[Y] \end{aligned}
$$
为了直观体现协方差的性质, 取 $X$ 和 $Y$ 为事件 $A$ 和 $B$   是否发生的示性函数.
$$
\operatorname{Cov}(X, Y)=\mathrm{P}\{X=1, Y=1\}-\mathrm{P}\{X=1\} \mathrm{P}\{Y=1\}
$$
所以
$$
\begin{aligned} \operatorname{Cov}(X, Y)>0 & \Leftrightarrow \mathrm{P}\{X=1, Y=1\}>\{X=1\} \mathrm{P}\{Y=1\} \\ & \Leftrightarrow \frac{\mathrm{P}\{X=1, Y=1\}}{P\{X=1\}}>\mathrm{P}\{Y=1\} \\ & \Leftrightarrow \mathrm{P}\{Y=1 | X=1\}>\mathrm{P}\{Y=1\} \end{aligned}
$$
就是说 $\operatorname{Cov}(X, Y)>0$ 意味着当 $X$ 增加, $Y$ 倾向于增加.

